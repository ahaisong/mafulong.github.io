---
layout: post
category: 课堂笔记
title: 机器学习笔记
---

## 机器学习简介及线性回归模型

**1.机器学习的定义**

- 假设用P来评估计算机程序在某任务类T上的性能，若一个程序通过利用经验E在T中任务上获得了性能改善，则我们就说关于T和P，，改程序对E进行了学习。 —— [ Mitchell,1997 ]

**2.机器学习算法分类**

1. 监督学习：对于训练的数据集，标示明确的实际结果（如标明样本的房价，肿瘤的良性与恶性）。可分为：回归(连续值)和分类(离散值)
2. 非监督学习：在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。非监督学习中，数据将会被分为不同的cluster(簇),称为cluster algorithm。如新闻网页的专题分类。 

**3.单变量线性回归**

算法的工作原理如下图。用训练集“喂养”我们的学习算法，形成假设函数h。然后，对输入的x值，输出相应的预测值y。相当于是存在一个映射关系：y=f(x) 

![](https://img-blog.csdn.net/20170905204847129?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWVVORkVJWUFIRw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

代价函数：用来选择最合适的曲线。在假设函数hθ中,有两个未知量，选择不同的参数值，最终的效果肯定是不一样的

这样我们就得到了代价函数(cost function)。此处也称为平方误差函数，当然也有其他的代价函数。但是对于大多数问题，尤其是回归问题，平方误差函数都是一个合理的选择，“其可能是解决回归问题最常用的手段了”。 
如此，我们的目标就是要让代价函数最小

![](http://7xrrje.com1.z0.glb.clouddn.com/screenshot_11.png?imageMogr/v2/thumbnail/!45p)

**梯度下降算法**：

梯度下降算法是一种优化算法, 它可以帮助我们找到一个函数的局部极小值点。 

首先要知道方向导数、偏导数与梯度的概念。它们均涉及到函数的变化率，也就是增长的问题。对于高维函数，偏导数只是函数在坐标轴方向的变化率，但是很明显，函数可以有无数个方向（在xy平面内考虑）的变化，也就是方向导数。梯度，则是这所有中函数值增长最快的方向，考虑山丘地形，意味着最陡峭的地方。那么，我们的代价函数目标则是求最小值，只要沿着梯度相反方向，就可以最快到达目的地了。 

算法描述： 

![](http://7xrrje.com1.z0.glb.clouddn.com/screenshot_21.png?imageMogr/v2/thumbnail/!45p)

## 多元线性回归及正规公式

|梯度下降	|正规方程式|
| ------ | ------ | ------ |
|需要选择步长α	|不需要选择步长α|
|需要迭代训练很多次	|一次都不需要迭代训练|
|O(kn2)	|O(n3,计算(XT·X)-1需要花费较长时间|

即使数据特征n很大，也可以正常工作	n如果过大，计算会消耗大量时间